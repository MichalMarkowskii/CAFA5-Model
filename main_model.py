# -*- coding: utf-8 -*-
"""main_zapas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19nQCQ2ZiMzeTZvRDMg2X37NE76HR4uak
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install obonet
!pip install Bio
!pip install peft

import obonet
from Bio import SeqIO
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import trange, tqdm
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import RocCurveDisplay, recall_score, precision_score, f1_score
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, logging
from peft import LoraConfig, get_peft_model
from google.colab import drive
drive.mount('/content/drive')
# %cd '/content/drive/My Drive'
logging.set_verbosity_error()

np.random.seed(30)
mlb = MultiLabelBinarizer()

# Wybór najczęstszych białek i funkcji (tutaj 10000 najczęstszych białek i 1000 najczęstszych funkcji GO)
train_terms = pd.read_csv("train_terms.tsv", sep="\t")
functions = train_terms.groupby("EntryID")["term"].apply(list)
functions_df = pd.DataFrame(mlb.fit_transform(functions), columns=mlb.classes_, index=functions.index)
functions_df = functions_df[functions_df.sum(axis=0).sort_values(ascending=False)[:1000].index]
functions_df = functions_df.loc[functions_df.sum(axis=1).sort_values(ascending=False)[:10000].index]
functions_df = functions_df[np.random.permutation(functions_df.columns)].sample(frac=1)
functions = functions_df.apply(lambda row: row.index[row == 1].tolist(), axis=1).to_dict()

# Opisy funkcji GO
go_graph = obonet.read_obo("go-basic.obo")
go_to_desc = {id_: (data.get("namespace") +
                    data.get("def")).replace("_", " ").replace('"', " ")
                    for id_, data in go_graph.nodes(data=True)}
go_to_desc = {k: go_to_desc[k] for k in functions_df.columns}

# Sekwencje aminokwasowe każdego białka
protein_to_seq = SeqIO.to_dict(SeqIO.parse(open("train_sequences.fasta"), "fasta"))
protein_to_seq = {k: protein_to_seq[k] for k in functions_df.index}

class ProteinsDataset(Dataset):

    def __init__(self, protein_to_seq, go_to_desc, functions):

        self.protein_list = list(protein_to_seq.keys())
        self.protein_to_seq = protein_to_seq
        self.go_list = list(go_to_desc.keys())
        self.go_to_desc = go_to_desc
        self.functions = functions

        self.protein_tokenizer = AutoTokenizer.from_pretrained("yarongef/DistilProtBert")
        self.go_tokenizer = AutoTokenizer.from_pretrained("nlpie/distil-biobert")

    def __len__(self):
        return len(self.protein_to_seq) * len(self.go_to_desc)

    def __getitem__(self, idx):

        # Wybranie żądanej sekwencji białka
        protein_idx = idx // len(self.go_to_desc)
        protein_name = self.protein_list[protein_idx]
        protein_sequence = " ".join(str(self.protein_to_seq[protein_name].seq))

        # Wybranie żądanego opisu funkcji GO
        go_idx = idx - protein_idx * len(self.go_to_desc)
        go_name = self.go_list[go_idx]
        go_description = self.go_to_desc[go_name]

        # Ustalenie czy funkcja jest związana z białkiem (1.0 lub 0.0)
        label = float(go_name in self.functions[protein_name])

        # Tokenizacja sekwencji
        protein_inputs = self.protein_tokenizer.encode_plus(
            protein_sequence, padding="max_length", max_length=1024,
            truncation=True, return_tensors="pt"
        )
        go_inputs = self.go_tokenizer.encode_plus(
            go_description, padding="max_length", max_length=256,
            truncation=True, return_tensors="pt"
        )

        protein_input_ids = protein_inputs["input_ids"].squeeze()
        protein_attention_mask = protein_inputs["attention_mask"].squeeze()
        go_input_ids = go_inputs["input_ids"].squeeze()
        go_attention_mask = go_inputs["attention_mask"].squeeze()

        return (protein_input_ids, protein_attention_mask,
                go_input_ids, go_attention_mask, label)

batch_size = 32

# Liczba białek i funkcji brana pod uwagę w zestawie treningowym
n_prot = 30
n_func = 2

train_dataset = ProteinsDataset(
    {k: protein_to_seq[k] for k in list(protein_to_seq)[:n_prot]},
    {k: go_to_desc[k] for k in list(go_to_desc)[:n_func]},
    functions
)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Liczba białek i funkcji brana pod uwagę w zestawie walidacyjnym
n_prot_val = 20
n_func_val = 20

val_dataset = ProteinsDataset(
    {k: protein_to_seq[k] for k in list(protein_to_seq)[n_prot:n_prot+n_prot_val]},
    {k: go_to_desc[k] for k in list(go_to_desc)[n_func:n_func+n_func_val]},
    functions
)

val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

class ProteinPredictor(nn.Module):

    def __init__(self):
        super(ProteinPredictor, self).__init__()

        # Załadowanie modeli
        self.protein_model = AutoModel.from_pretrained("yarongef/DistilProtBert")
        self.go_model = AutoModel.from_pretrained("nlpie/distil-biobert")

        # Pobranie rozmiarów osadzeń modeli
        self.protein_embedding_dim = self.protein_model.config.hidden_size
        self.go_embedding_dim = self.go_model.config.hidden_size

        # Zmiana rozmiarów osadzeń tak aby były one takie same
        self.protein_reshape = nn.Linear(self.protein_embedding_dim, 256)
        self.go_reshape = nn.Linear(self.go_embedding_dim, 256)

        # Końcowy MLP zwracający końcowy logit określający prawdopodobieństwo
        self.mlp = nn.Sequential(
            nn.Linear(3 * 256, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 1)
        )

    def forward(self, protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask):

        # Generacja osadzeń białek
        protein_outputs = self.protein_model(input_ids=protein_input_ids,
                                             attention_mask=protein_attention_mask)
        # Average pooling
        protein_embeddings = protein_outputs.last_hidden_state.mean(dim=1)

        # Generate osadzeń funkcji GO
        go_outputs = self.go_model(input_ids=go_input_ids,
                                   attention_mask=go_attention_mask)
        # Average pooling
        go_embeddings = go_outputs.last_hidden_state.mean(dim=1)

        # Zmiana rozmiaru
        protein_embeddings_reshaped = torch.tanh(self.protein_reshape(protein_embeddings))
        go_embeddings_reshaped = torch.tanh(self.go_reshape(go_embeddings))

        # Konkatenacja osadzeń w formie: osadzenia białkowe, osadzenia funkcyjne, osadzenia będące wartością bezwzględną obu poprzednich osadzeń
        combined_embeddings = torch.cat((protein_embeddings_reshaped,
                                         go_embeddings_reshaped,
                                         torch.absolute(protein_embeddings_reshaped - go_embeddings_reshaped)), dim=1)

        # Przepuszczenie przez MLP w celu predykcji
        output = self.mlp(combined_embeddings)

        return output

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Konfiguracja LoRA (wybieram co drugą warstwę z DistilBioBERT i 2 ostatnie warstwy z DistilProtBERT)
config = LoraConfig(
    r=8,
    lora_dropout=0.1,
    target_modules=[
        "go_model.encoder.layer.0.output.dense",
        "go_model.encoder.layer.3.output.dense",
        "go_model.encoder.layer.5.output.dense",
        "protein_model.encoder.layer.13.output.dense",
        "protein_model.encoder.layer.14.output.dense"],
    modules_to_save=[
        "protein_reshape",
        "go_reshape",
        "mlp.0",
        "mlp.2"]
)

model = ProteinPredictor()
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()
model = peft_model.to(device)

# Proporcja próbek pozytywnych ('positive samples')
weight = functions_df.iloc[:n_prot][functions_df.columns[:n_func]].mean().mean()

# Współczynnik określający jak dużo próbek negatywnych przypada na 1 pozytywną (waga do funkcji straty)
pos_weight = (1 - weight) / weight

optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, amsgrad=True)
criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))
num_epochs = 3

# Mixed Precision
scaler = torch.cuda.amp.GradScaler(enabled=True)
bar = trange(num_epochs)

loss_history = []
val_loss_history = []

for epoch in bar:

    # Trening
    model.train()
    for protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask, labels in train_dataloader:

        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=True):

            protein_input_ids = protein_input_ids.to(device)
            protein_attention_mask = protein_attention_mask.to(device)
            go_input_ids = go_input_ids.to(device)
            go_attention_mask = go_attention_mask.to(device)
            labels = labels.to(device)

            outputs = model(protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask)
            loss = criterion(outputs, labels.unsqueeze(1).float())

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()

        loss_history.append(loss.item())
        bar.set_postfix(loss=f"{np.mean(loss_history[-train_dataset.__len__()//batch_size:]):,.3f}")

    # Walidacja
    model.eval()
    for protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask, labels in val_dataloader:

        val_loss = []

        with torch.no_grad():

            protein_input_ids = protein_input_ids.to(device)
            protein_attention_mask = protein_attention_mask.to(device)
            go_input_ids = go_input_ids.to(device)
            go_attention_mask = go_attention_mask.to(device)
            labels = labels.to(device)

            outputs = model(protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask)
            loss = criterion(outputs, labels.unsqueeze(1).float())

            val_loss.append(loss.item())

        val_loss_history.append(np.mean(val_loss))

import matplotlib.pyplot as plt

z_train = np.polyfit(range(len(loss_history)), loss_history, 1)
p_train = np.poly1d(z_train)
z_val = np.polyfit(range(len(val_loss_history)), val_loss_history, 1)
p_val = np.poly1d(z_val)
print(sum(loss_history)/len(loss_history))
print(sum(val_loss_history)/len(val_loss_history))

fig, ax = plt.subplots()
plt.plot(loss_history, label='Training Loss')
plt.plot(p_train(range(len(loss_history))), "--", label='Training Loss Trend')
plt.title('Training Loss')
plt.xlabel('Step')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
fig.savefig(f"/content/training_loss_np{n_prot}_nf{n_func}.eps", format='eps')

fig, ax = plt.subplots()
plt.plot(val_loss_history, label='Validation Loss')
plt.plot(p_val(range(len(val_loss_history))), "--", label='Validation Loss Trend')
plt.title('Validation Loss')
plt.xlabel('Step')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
fig.savefig(f"/content/validation_loss_np{n_prot}_nf{n_func}.eps", format='eps')

def split_by_epochs(data, epochs):
    split_data = np.array_split(data, epochs)
    return split_data

train_data_split = split_by_epochs(loss_history, num_epochs)
val_data_split = split_by_epochs(val_loss_history, num_epochs)

submean_train_data = [np.mean(subarray) for subarray in train_data_split]
submean_val_data = [np.mean(subarray) for subarray in val_data_split]
mean_train_data = np.mean(train_data_split)
mean_val_data = np.mean(val_data_split)
print(submean_train_data)
print(submean_val_data)
print(mean_train_data)
print(mean_val_data)


positions_train = np.arange(1, num_epochs * 2, 2)  # Odd positions for training
positions_val = np.arange(2, num_epochs * 2 + 1, 2)  # Even positions for validation

fig, ax = plt.subplots(figsize=(10, 6))

# Plotting the box plots without 'label' argument
bp_train = ax.boxplot(train_data_split, positions=positions_train, patch_artist=True, boxprops=dict(facecolor="lightblue"))
bp_val = ax.boxplot(val_data_split, positions=positions_val, patch_artist=True, boxprops=dict(facecolor="lightgreen"))

# Adding labels and title
ax.set_title('Training and Validation Loss per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')

# Customizing the x-axis to show epoch numbers
epoch_labels = [(f"{x+1}") for x in range(num_epochs)]
ax.set_xticks(np.arange(1, num_epochs * 2, 2))
ax.set_xticklabels(epoch_labels)

# Adding legend manually using the patch colors
ax.legend([bp_train["boxes"][0], bp_val["boxes"][0]], ['Training Loss', 'Validation Loss'])
fig.savefig(f"/content/box_loss_np{n_prot}_nf{n_func}.eps", format='eps')

plt.tight_layout()
plt.show()

# Predykcje na widzianych białkach i widzianych funkcjach

model.eval()
y_pred = []
y_true = []

for i in tqdm(range(train_dataset.__len__() // 5)):

    with torch.no_grad():

        protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask, label = train_dataset.__getitem__(i)

        protein_input_ids = torch.reshape(protein_input_ids, (1, 1024)).to(device)
        protein_attention_mask = torch.reshape(protein_attention_mask, (1, 1024)).to(device)
        go_input_ids = torch.reshape(go_input_ids, (1, 256)).to(device)
        go_attention_mask = torch.reshape(go_attention_mask, (1, 256)).to(device)

        output = model(protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask)
        y_true.append(label)
        y_pred.append(torch.sigmoid(output).item())

roc_display = RocCurveDisplay.from_predictions(y_true, y_pred)
plt.title("ROC Curve")
plt.savefig(f"/content/roc_p1_f1_np{n_prot}_nf{n_func}.eps", format='eps')
plt.show()
print(f"Recall: {recall_score(y_true, (np.asarray(y_pred) > 0.5).astype(int)):.3f}")
print(f"Precision: {precision_score(y_true, (np.asarray(y_pred) > 0.5).astype(int)):.3f}")
print(f"F1-Score: {f1_score(y_true, (np.asarray(y_pred) > 0.5).astype(int)):.3f}")

# Widziane białka, niewidziane funkcje

test_dataset = ProteinsDataset({k: protein_to_seq[k] for k in list(protein_to_seq)[:n_prot]},
                               {k: go_to_desc[k] for k in list(go_to_desc)[n_func:2*n_func]},
                               functions)

test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

model.eval()
y_pred_2 = []
y_true_2 = []

for i in tqdm(range(test_dataset.__len__() // 5)):

    with torch.no_grad():

        protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask, label = test_dataset.__getitem__(i)

        protein_input_ids = torch.reshape(protein_input_ids, (1, 1024)).to(device)
        protein_attention_mask = torch.reshape(protein_attention_mask, (1, 1024)).to(device)
        go_input_ids = torch.reshape(go_input_ids, (1, 256)).to(device)
        go_attention_mask = torch.reshape(go_attention_mask, (1, 256)).to(device)

        output = model(protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask)
        y_true_2.append(label)
        y_pred_2.append(torch.sigmoid(output).item())

roc_display = RocCurveDisplay.from_predictions(y_true_2, y_pred_2)
plt.title("ROC Curve")
plt.savefig(f"/content/roc_p1_f0_np{n_prot}_nf{n_func}.eps", format='eps')
plt.show()
print(f"Recall: {recall_score(y_true_2, (np.asarray(y_pred_2) > 0.5).astype(int)):.3f}")
print(f"Precision: {precision_score(y_true_2, (np.asarray(y_pred_2) > 0.5).astype(int)):.3f}")
print(f"F1-Score: {f1_score(y_true_2, (np.asarray(y_pred_2) > 0.5).astype(int)):.3f}")

# Niewidziane białka, widziane funkcje

test_dataset = ProteinsDataset({k: protein_to_seq[k] for k in list(protein_to_seq)[n_prot:2*n_prot]},
                               {k: go_to_desc[k] for k in list(go_to_desc)[:n_func]},
                               functions)

test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

model.eval()
y_pred_3 = []
y_true_3 = []

for i in tqdm(range(test_dataset.__len__() // 5)):

    with torch.no_grad():

        protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask, label = test_dataset.__getitem__(i)

        protein_input_ids = torch.reshape(protein_input_ids, (1, 1024)).to(device)
        protein_attention_mask = torch.reshape(protein_attention_mask, (1, 1024)).to(device)
        go_input_ids = torch.reshape(go_input_ids, (1, 256)).to(device)
        go_attention_mask = torch.reshape(go_attention_mask, (1, 256)).to(device)

        output = model(protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask)
        y_true_3.append(label)
        y_pred_3.append(torch.sigmoid(output).item())

roc_display = RocCurveDisplay.from_predictions(y_true_3, y_pred_3)
plt.title("ROC Curve")
plt.savefig(f"/content/roc_p0_f1_np{n_prot}_nf{n_func}.eps", format='eps')
plt.show()
print(f"Recall: {recall_score(y_true_3, (np.asarray(y_pred_3) > 0.5).astype(int)):.3f}")
print(f"Precision: {precision_score(y_true_3, (np.asarray(y_pred_3) > 0.5).astype(int)):.3f}")
print(f"F1-Score: {f1_score(y_true_3, (np.asarray(y_pred_3) > 0.5).astype(int)):.3f}")

# Niewidziane białka, Nie widziane funkcje

test_dataset = ProteinsDataset({k: protein_to_seq[k] for k in list(protein_to_seq)[n_prot:2*n_prot]},
                               {k: go_to_desc[k] for k in list(go_to_desc)[n_func:2*n_func]},
                               functions)

test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

model.eval()
y_pred_4 = []
y_true_4 = []

for i in tqdm(range(test_dataset.__len__() // 5)):

    with torch.no_grad():

        protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask, label = test_dataset.__getitem__(i)

        protein_input_ids = torch.reshape(protein_input_ids, (1, 1024)).to(device)
        protein_attention_mask = torch.reshape(protein_attention_mask, (1, 1024)).to(device)
        go_input_ids = torch.reshape(go_input_ids, (1, 256)).to(device)
        go_attention_mask = torch.reshape(go_attention_mask, (1, 256)).to(device)

        output = model(protein_input_ids, protein_attention_mask, go_input_ids, go_attention_mask)
        y_true_4.append(label)
        y_pred_4.append(torch.sigmoid(output).item())

roc_display = RocCurveDisplay.from_predictions(y_true_4, y_pred_4)
plt.title("ROC Curve")
plt.savefig(f"/content/roc_p0_f0_np{n_prot}_nf{n_func}.eps", format='eps')
plt.show()
print(f"Recall: {recall_score(y_true_4, (np.asarray(y_pred_4) > 0.5).astype(int)):.3f}")
print(f"Precision: {precision_score(y_true_4, (np.asarray(y_pred_4) > 0.5).astype(int)):.3f}")
print(f"F1-Score: {f1_score(y_true_4, (np.asarray(y_pred_4) > 0.5).astype(int)):.3f}")

fig, ax = plt.subplots()

# Plot ROC curve for each set of predictions
RocCurveDisplay.from_predictions(y_true, y_pred, ax=ax, name='B1F1')
RocCurveDisplay.from_predictions(y_true_2, y_pred_2, ax=ax, name='B1F0')
RocCurveDisplay.from_predictions(y_true_3, y_pred_3, ax=ax, name='B0F1')
RocCurveDisplay.from_predictions(y_true_4, y_pred_4, ax=ax, name='B0F0')

# Set the title and show the plot
plt.title("ROC Curve Comparison")
plt.savefig(f"/content/roc_full_np{n_prot}_nf{n_func}.eps", format='eps')
plt.show()